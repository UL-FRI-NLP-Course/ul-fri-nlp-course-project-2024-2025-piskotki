\begin{thebibliography}{1}

\bibitem{bang}
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie,
  Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet~V. Do, Yan Xu, and
  Pascale Fung.
\newblock A multitask, multilingual, multimodal evaluation of chatgpt on
  reasoning, hallucination, and interactivity, 2023.

\bibitem{cao}
Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi~Kit Cheung.
\newblock Factual error correction for abstractive summarization models, 2021.

\bibitem{guu}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
\newblock Realm: Retrieval-augmented language model pre-training, 2020.

\bibitem{gao}
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai,
  Jiawei Sun, Meng Wang, and Haofen Wang.
\newblock Retrieval-augmented generation for large language models: A survey,
  2024.

\bibitem{chen}
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le~Sun.
\newblock Benchmarking large language models in retrieval-augmented generation,
  2023.

\end{thebibliography}
